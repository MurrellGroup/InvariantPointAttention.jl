var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = InvariantPointAttention","category":"page"},{"location":"#InvariantPointAttention","page":"Home","title":"InvariantPointAttention","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for InvariantPointAttention.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [InvariantPointAttention]","category":"page"},{"location":"#Core.Union-NTuple{4, Any}","page":"Home","title":"Core.Union","text":"Cross IPA Partial Structure Module - single layer - adapted from AF2. From left to right. \n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.BackboneUpdate","page":"Home","title":"InvariantPointAttention.BackboneUpdate","text":"Projects the frame embedding => 6, and uses this to transform the input frames.\n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPA","page":"Home","title":"InvariantPointAttention.IPA","text":"Strictly Self-IPA initialization\n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPACache-Tuple{NamedTuple, Integer}","page":"Home","title":"InvariantPointAttention.IPACache","text":"IPACache(settings, batchsize)\n\nInitialize an empty IPA cache.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.IPCrossA","page":"Home","title":"InvariantPointAttention.IPCrossA","text":"Invariant Point Cross Attention (IPCrossA). Information flows from L (Keys, Values) to R (Queries).\n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPCrossAStructureModuleLayer","page":"Home","title":"InvariantPointAttention.IPCrossAStructureModuleLayer","text":"Cross IPA Partial Structure Module initialization - single layer - adapted from AF2. From left to right. \n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPA_settings-Tuple{Any}","page":"Home","title":"InvariantPointAttention.IPA_settings","text":"Returns a tuple of the IPA settings, with defaults for everything except dims. This can be passed to the IPA and IPCrossAStructureModuleLayer.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.T_R3-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, AbstractArray{T, N}, AbstractArray{T, N}}} where {T, N}","page":"Home","title":"InvariantPointAttention.T_R3","text":"Applies the SE3 transformations T = (rot,trans) ∈ SE(3)^N to N batches of m points in R3, i.e., mat ∈ R^(3 x m x N) ↦ T(mat) ∈ R^(3 x m x N). Note here that rotations here are represented in matrix form. \n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.T_R3_inv-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, AbstractArray{T, N}, AbstractArray{T, N}}} where {T, N}","page":"Home","title":"InvariantPointAttention.T_R3_inv","text":"Applies the group inverse of the SE3 transformations T = (R,t) ∈ SE(3)^N to N batches of m points in R3, such that T^-1(T*x) = T^-1(Rx+t) =  R^T(Rx+t-t) = x.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.T_T-Tuple{Any, Any}","page":"Home","title":"InvariantPointAttention.T_T","text":"Returns the composition of two SE(3) transformations T1 and T2. If T1 = (R1,t1), and T2 = (R2,t2) then T1T2 = (R1R2, R1*t2 + t1).\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.T_till-Tuple{Any, Any}","page":"Home","title":"InvariantPointAttention.T_till","text":"Index into a T up to index i. \n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.bcds2quats-Union{Tuple{AbstractMatrix{T}}, Tuple{T}, Tuple{AbstractMatrix{T}, T}} where T<:Real","page":"Home","title":"InvariantPointAttention.bcds2quats","text":"Takes a 3xN matrix of imaginary quaternion components, bcd, sets the real part to a, and normalizes to unit quaternions.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.calculate_residue_rotation_and_translation-Tuple{AbstractMatrix}","page":"Home","title":"InvariantPointAttention.calculate_residue_rotation_and_translation","text":"Get frame from residue\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_T-Tuple{Array{<:Real, 3}}","page":"Home","title":"InvariantPointAttention.get_T","text":"Get the assosciated SE(3) frame for all residues in a prot\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_T_batch-Tuple{Array{<:Real, 4}}","page":"Home","title":"InvariantPointAttention.get_T_batch","text":"Get the assosciated SE(3) frames for all residues in a batch of prots \n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_rotation-Tuple{Type{<:Real}, Vararg{Any}}","page":"Home","title":"InvariantPointAttention.get_rotation","text":"get_rotation([T=Float32,] dims...)\n\nGenerates random rotation matrices of given size.  \n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_translation-Tuple{Type{<:Real}, Vararg{Any}}","page":"Home","title":"InvariantPointAttention.get_translation","text":"get_translation([T=Float32,] dims...)\n\nGenerates random translations of given size.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.left_to_right_mask-Tuple{Type{<:AbstractFloat}, Integer, Integer}","page":"Home","title":"InvariantPointAttention.left_to_right_mask","text":"left_to_right_mask([T=Float32,] L::Integer, R::Integer; step::Integer = 10)\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.right_to_left_mask-Tuple{Type{<:AbstractFloat}, Integer, Integer}","page":"Home","title":"InvariantPointAttention.right_to_left_mask","text":"right_to_left_mask([T=Float32,] L::Integer, R::Integer; step::Integer = 10)\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.right_to_left_mask-Tuple{Type{<:AbstractFloat}, Integer}","page":"Home","title":"InvariantPointAttention.right_to_left_mask","text":"right_to_left_mask([T=Float32,] N::Integer)\n\nCreate a right-to-left mask for the self-attention mechanism. The mask is a matrix of size N x N where the diagonal and the lower triangular part are set to zero and the upper triangular part is set to infinity.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.rotmatrix_from_quat-Tuple{AbstractMatrix{<:Real}}","page":"Home","title":"InvariantPointAttention.rotmatrix_from_quat","text":"Takes a 4xN matrix of unit quaternions and returns a 3x3xN array of rotation matrices.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.softmax1-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T","page":"Home","title":"InvariantPointAttention.softmax1","text":"softmax1(x, dims = 1)\n\nBehaves like softmax, but as though there was an additional logit of zero along dims (which is excluded from the output). So the values will sum to a value between zero and 1.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.update_frame-Tuple{Any, Any}","page":"Home","title":"InvariantPointAttention.update_frame","text":"Takes a 6-dim vec and maps to a rotation matrix and translation vector, which is then applied to the input frames.\n\n\n\n\n\n","category":"method"}]
}
