var documenterSearchIndex = {"docs":
[{"location":"#InvariantPointAttention","page":"Home","title":"InvariantPointAttention","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for InvariantPointAttention.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#InvariantPointAttention.BackboneUpdate","page":"Home","title":"InvariantPointAttention.BackboneUpdate","text":"Projects the frame embedding => 6, and uses this to transform the input frames.\n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPA","page":"Home","title":"InvariantPointAttention.IPA","text":"Strictly Self-IPA initialization\n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPACache-Tuple{NamedTuple, Integer}","page":"Home","title":"InvariantPointAttention.IPACache","text":"IPACache(settings, batchsize)\n\nInitialize an empty IPA cache.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.IPAStructureModuleLayer","page":"Home","title":"InvariantPointAttention.IPAStructureModuleLayer","text":"Self IPA Partial Structure Module initialization - single layer - adapted from AF2. \n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPCrossA","page":"Home","title":"InvariantPointAttention.IPCrossA","text":"IPCrossA(settings)\n\nInvariant Point Cross Attention (IPCrossA). Information flows from L (Keys, Values) to R (Queries).\n\nGet settings with IPA_settings\n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPCrossAStructureModuleLayer","page":"Home","title":"InvariantPointAttention.IPCrossAStructureModuleLayer","text":"Cross IPA Partial Structure Module initialization - single layer - adapted from AF2. From left to right. \n\n\n\n\n\n","category":"type"},{"location":"#InvariantPointAttention.IPA_settings-Tuple{Any}","page":"Home","title":"InvariantPointAttention.IPA_settings","text":"IPA_settings(\n    dims;\n    c = 16,\n    N_head = 12,\n    N_query_points = 4,\n    N_point_values = 8,\n    c_z = 0,\n    Typ = Float32,\n    use_softmax1 = false,\n    scaling_qk = :default,\n)\n\nReturns a tuple of the IPA settings, with defaults for everything except dims. This can be passed to the IPA and IPCrossAStructureModuleLayer.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.dotproducts-Union{Tuple{T}, Tuple{IPARoPE, AbstractArray{T, 4}, AbstractArray{T, 4}}} where T<:Real","page":"Home","title":"InvariantPointAttention.dotproducts","text":"function RoPEdotproducts(iparope::IPARoPE, q, k; chain_diffs = nothing)\n\nchain_diffs is either nothing or a array of 0's and 1's describing the ij-pair as pertaining to the same chain if the entry at ij is 1, else 0.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_T-Tuple{Array{<:Real, 3}}","page":"Home","title":"InvariantPointAttention.get_T","text":"get_T(coords::Array{<:Real, 3})\n\nGet the assosciated SE(3) frame for all residues in a protein backbone represented as a 3x3xL array of coordinates.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_T_batch-Tuple{Array{<:Real, 4}}","page":"Home","title":"InvariantPointAttention.get_T_batch","text":"Get the associated SE(3) frames for all residues in a batch of proteins\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_rotation-Tuple{Type{<:Real}, Vararg{Any}}","page":"Home","title":"InvariantPointAttention.get_rotation","text":"get_rotation([T=Float32,] dims...)\n\nGenerates random rotation matrices of given size.  \n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.get_translation-Tuple{Type{<:Real}, Vararg{Any}}","page":"Home","title":"InvariantPointAttention.get_translation","text":"get_translation([T=Float32,] dims...)\n\nGenerates random translations of given size.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.left_to_right_mask-Tuple{Type{<:AbstractFloat}, Integer, Integer}","page":"Home","title":"InvariantPointAttention.left_to_right_mask","text":"left_to_right_mask([T=Float32,] L::Integer, R::Integer; step::Integer = 10)\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.right_to_left_mask-Tuple{Type{<:AbstractFloat}, Integer, Integer}","page":"Home","title":"InvariantPointAttention.right_to_left_mask","text":"right_to_left_mask([T=Float32,] L::Integer, R::Integer; step::Integer = 10)\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.right_to_left_mask-Tuple{Type{<:AbstractFloat}, Integer}","page":"Home","title":"InvariantPointAttention.right_to_left_mask","text":"right_to_left_mask([T=Float32,] N::Integer)\n\nCreate a right-to-left mask for the self-attention mechanism. The mask is a matrix of size N x N where the diagonal and the lower triangular part are set to zero and the upper triangular part is set to infinity.\n\n\n\n\n\n","category":"method"},{"location":"#InvariantPointAttention.softmax1-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T","page":"Home","title":"InvariantPointAttention.softmax1","text":"softmax1(x, dims = 1)\n\nBehaves like softmax, but as though there was an additional logit of zero along dims (which is excluded from the output). So the values will sum to a value between zero and 1.\n\nSee https://www.evanmiller.org/attention-is-off-by-one.html\n\n\n\n\n\n","category":"method"}]
}
